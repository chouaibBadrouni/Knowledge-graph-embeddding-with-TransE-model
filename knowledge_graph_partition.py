# -*- coding: utf-8 -*-
"""Knowledge graph partition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-hEU7U4u6SRoD1Nt6u8UFagnX3tyOagl
"""

pip install torchkge

from torch import cuda
from torch.optim import Adam

from torchkge.models import TransEModel
from torchkge.sampling import BernoulliNegativeSampler
from torchkge.utils import MarginLoss, DataLoader
from torchkge.utils.datasets import load_fb15k

from tqdm.autonotebook import tqdm

# Load dataset
kg_train, _, _ = load_fb15k()

# Define some hyper-parameters for training
emb_dim = 100
lr = 0.0004
n_epochs = 10
b_size = 32768
margin = 0.5

# Define the model and criterion
model = TransEModel(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')
criterion = MarginLoss(margin)

# Move everything to CUDA if available
if cuda.is_available():
    cuda.empty_cache()
    model.cuda()
    criterion.cuda()

# Define the torch optimizer to be used
optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)

sampler = BernoulliNegativeSampler(kg_train)
dataloader = DataLoader(kg_train, batch_size=b_size, use_cuda='cpu')

iterator = tqdm(range(n_epochs), unit='epoch')
for epoch in iterator:
    running_loss = 0.0
    for i, batch in enumerate(dataloader):
        h, t, r = batch[0], batch[1], batch[2]
        n_h, n_t = sampler.corrupt_batch(h, t, r)

        optimizer.zero_grad()

        # forward + backward + optimize
        pos, neg = model(h, t, r, n_h, n_t)
        loss = criterion(pos, neg)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    iterator.set_description(
        'Epoch {} | mean loss: {:.5f}'.format(epoch + 1,
                                              running_loss / len(dataloader)))

model.normalize_parameters()

pip install torchkge

from torch.optim import Adam

from torchkge.evaluation import LinkPredictionEvaluator
from torchkge.models import TransEModel
from torchkge.utils.datasets import load_fb15k
from torchkge.utils import Trainer, MarginLoss


def main():
    # Define some hyper-parameters for training
    emb_dim = 100
    lr = 0.0004
    margin = 0.5
    n_epochs = 10
    batch_size = 32768

    # Load dataset
    kg_train, kg_val, kg_test = load_fb15k()

    # Define the model and criterion
    model = TransEModel(emb_dim, kg_train.n_ent, kg_train.n_rel,
                        dissimilarity_type='L2')
    criterion = MarginLoss(margin)
    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)

    trainer = Trainer(model, criterion, kg_train, n_epochs, batch_size,
                      optimizer=optimizer, sampling_type='bern', use_cuda='all',)

    trainer.run()

    evaluator = LinkPredictionEvaluator(model, kg_test)
    evaluator.evaluate(200)
    evaluator.print_results()


if __name__ == "__main__":
    main()

pip install ignite

pip install --upgrade ignite

pip install torch ignite

pip install pytorch-ignite

import torch
from ignite.engine import Engine, Events
from ignite.handlers import EarlyStopping
from ignite.metrics import RunningAverage
from torch.optim import Adam

from torchkge.evaluation import LinkPredictionEvaluator
from torchkge.models import TransEModel
from torchkge.sampling import BernoulliNegativeSampler
from torchkge.utils import MarginLoss, DataLoader
from torchkge.utils.datasets import load_fb15k


def process_batch(engine, batch):
    h, t, r = batch[0].to("cuda:0"), batch[1].to("cuda:0"), batch[2].to("cuda:0")
    n_h, n_t = sampler.corrupt_batch(h.cpu(), t.cpu(), r.cpu())
    n_h, n_t, r = n_h.to("cuda:0"), n_t.to("cuda:0"), r.to("cuda:0")
    pos, neg = model(h, t, r), model(n_h, n_t, r)
    loss = criterion(pos, neg)
    loss.backward()
    optimizer.step()
    return loss.item()



def linkprediction_evaluation(engine):
    model.normalize_parameters()

    loss = engine.state.output

    # validation MRR measure
    if engine.state.epoch % eval_epoch == 0:
        evaluator = LinkPredictionEvaluator(model, kg_val)
        evaluator.evaluate(b_size=256, verbose=False)
        val_mrr = evaluator.mrr()[1]
    else:
        val_mrr = 0

    print('Epoch {} | Train loss: {}, Validation MRR: {}'.format(
        engine.state.epoch, loss, val_mrr))

    try:
        if engine.state.best_mrr < val_mrr:
            engine.state.best_mrr = val_mrr
        return val_mrr

    except AttributeError as e:
        if engine.state.epoch == 1:
            engine.state.best_mrr = val_mrr
            return val_mrr
        else:
            raise e

device = torch.device('cuda')

eval_epoch = 20  # do link prediction evaluation each 20 epochs
max_epochs = 1000
patience = 40
batch_size = 32768
emb_dim = 100
lr = 0.0004
margin = 0.5

kg_train, kg_val, kg_test = load_fb15k()

# Define the model, optimizer and criterion
model = TransEModel(emb_dim, kg_train.n_ent, kg_train.n_rel,
                    dissimilarity_type='L2')
model.to(device)

optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)
criterion = MarginLoss(margin)
sampler = BernoulliNegativeSampler(kg_train, kg_val=kg_val, kg_test=kg_test)

# Define the engine
trainer = Engine(process_batch)

# Define the moving average
RunningAverage(output_transform=lambda x: x).attach(trainer, 'margin')

# Add early stopping
handler = EarlyStopping(patience=patience,
                        score_function=linkprediction_evaluation,
                        trainer=trainer)
trainer.add_event_handler(Events.EPOCH_COMPLETED, handler)

# Training
train_iterator = DataLoader(kg_train, batch_size, use_cuda='all')
trainer.run(train_iterator,
            epoch_length=len(train_iterator),
            max_epochs=max_epochs)

print('Best score {:.3f} at epoch {}'.format(handler.best_score,
                                             trainer.state.epoch - handler.patience))

pip install ignite

pip install torch_geometric

pip install torch_sparse torch_scatter

!pip install ogb
!python -c "import torch; print(torch.__version__)"
!python -c "import torch; print(torch.version.cuda)"
!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html
!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html
!pip install torch-geometric
!pip install -q git+https://github.com/snap-stanford/deepsnap.git

import numpy as np
import ogb
import os
import pdb
import random
import torch
import torch_geometric
import tqdm
from ogb.linkproppred import LinkPropPredDataset, PygLinkPropPredDataset
from torch.utils.data import DataLoader, Dataset

pip install datasets

import os.path as osp

import torch

from torch_geometric.datasets import FB15k_237
from torch_geometric.nn import TransE

device = 'cuda' if torch.cuda.is_available() else 'cpu'
path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'FB15k')

train_data = FB15k_237(path, split='train')[0].to(device)
val_data = FB15k_237(path, split='val')[0].to(device)
test_data = FB15k_237(path, split='test')[0].to(device)

model = TransE(
    num_nodes=train_data.num_nodes,
    num_relations=train_data.num_edge_types,
    hidden_channels=50,
).to(device)

loader = model.loader(
    head_index=train_data.edge_index[0],
    rel_type=train_data.edge_type,
    tail_index=train_data.edge_index[1],
    batch_size=10000,
    shuffle=True,
)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)


def train():
    model.train()
    total_loss = total_examples = 0
    for head_index, rel_type, tail_index in loader:
        optimizer.zero_grad()
        loss = model.loss(head_index, rel_type, tail_index)
        loss.backward()
        optimizer.step()
        total_loss += float(loss) * head_index.numel()
        total_examples += head_index.numel()
    return total_loss / total_examples


@torch.no_grad()
def test(data):
    model.eval()
    return model.test(
        head_index=data.edge_index[0],
        rel_type=data.edge_type,
        tail_index=data.edge_index[1],
        batch_size=20000,
        k=10,
    )


for epoch in range(1, 501):
    loss = train()
    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')
    if epoch % 25 == 0:
        rank, hits = test(val_data)
        print(f'Epoch: {epoch:03d}, Val Mean Rank: {rank:.2f}, '
              f'Val Hits@10: {hits:.4f}')

rank, hits_at_10 = test(test_data)
print(f'Test Mean Rank: {rank:.2f}, Test Hits@10: {hits_at_10:.4f}')

import networkx as nx
import matplotlib.pyplot as plt

# Créer un graphe simple
G = nx.Graph()
G.add_edges_from([(1, 2), (1, 3), (2, 3), (3, 4), (4, 5), (4, 6), (5, 6)])

# Détecter des communautés de noeuds
communities = nx.algorithms.community.greedy_modularity_communities(G)

# Afficher les communautés de noeuds
print("Communautés de noeuds détectées :")
for i, community in enumerate(communities):
    print("Communauté", i+1, ":", community)

# Dessiner le graphe avec les communautés de noeuds colorées
colors = ["red", "green", "blue"]
node_colors = []
for node in G.nodes():
    for i, community in enumerate(communities):
        if node in community:
            node_colors.append(colors[i])

nx.draw(G, with_labels=True, node_color=node_colors)
plt.show()

import networkx as nx
import pandas as pd
import matplotlib.pyplot as plt

# Chargement du dataset FB15K
df = pd.read_csv('/content/sample_data/train.csv', sep='\t', header=None, names=['s', 'p', 'o'])

# Création d'un graphe orienté à partir du dataset
G = nx.DiGraph()
df = pd.read_csv('path/to/FB15K.csv', delimiter='\t', header=None, names=['s', 'p', 'o'])


# Détection des communautés avec l'algorithme Label Propagation
communities = nx.algorithms.community.label_propagation.label_propagation_communities(G)

# Création d'un dictionnaire qui associe chaque relation triple à une communauté
community_dict = {}
for i, community in enumerate(communities):
    for node in community:
        community_dict[node] = i

# Assignation d'une couleur à chaque communauté
colors = ["red", "green", "blue"]
edge_colors = [colors[community_dict[(s, p, o)]] for s, p, o in G.edges()]

# Affichage du graphe avec les communautés de relations triples colorées
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos=pos, with_labels=True, node_size=1000, node_color="white", edge_color=edge_colors, width=2)
plt.show()

import networkx as nx
import random

# Créer un graphe vide avec 50 nœuds
G = nx.Graph()
G.add_nodes_from(range(50))

# Ajouter des arêtes entre chaque paire de nœuds
for i in range(50):
    for j in range(i+1, 50):
        if random.random() < 0.5:
            G.add_edge(i, j)

# Visualiser le graphe
nx.draw(G, with_labels=True)

import networkx as nx
import random

# Créer un graphe vide avec 50 nœuds
G = nx.Graph()
G.add_nodes_from(range(50))

# Ajouter des arêtes entre chaque paire de nœuds avec des poids aléatoires
for i in range(50):
    for j in range(i+1, 50):
        if random.random() < 0.5:
            weight = random.uniform(0.1, 1.0)
            G.add_edge(i, j, weight=weight)

# Visualiser le graphe avec les poids d'arêtes
pos = nx.spring_layout(G)
edge_labels = nx.get_edge_attributes(G, 'weight')
nx.draw(G, pos, with_labels=True)
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)

import networkx as nx

# Créer un graphe vide avec 50 nœuds
G = nx.Graph()
G.add_nodes_from(range(50))

# Ajouter des arêtes entre chaque paire de nœuds avec un poids de 0,5
for i in range(50):
    for j in range(i+1, 50):
        G.add_edge(i, j, weight=0.5)

# Visualiser le graphe avec les annotations de poids
pos = nx.spring_layout(G)
edge_labels = nx.get_edge_attributes(G, 'weight')
nx.draw(G, pos, with_labels=True)
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)

import networkx as nx

# Créer le graphe comme décrit précédemment
G = nx.Graph()
G.add_nodes_from(range(50))
for i in range(50):
    for j in range(i+1, 50):
        G.add_edge(i, j, weight=0.5)

# Calculer la pondération moyenne pour chaque paire de nœuds
avg_weight = nx.average_shortest_path_length(G, weight='weight')

# Afficher la pondération moyenne
print(f"La pondération moyenne des arêtes est {avg_weight:.2f}")

import networkx as nx
import random

# Créer un graphe vide avec 1450 nœuds
G = nx.Graph()
G.add_nodes_from(range(1450))

# Ajouter 137 arêtes aléatoires
for i in range(137):
    # Choisir deux nœuds au hasard
    u = random.randint(0, 1449)
    v = random.randint(0, 1449)

    # S'assurer que les nœuds ne sont pas déjà connectés
    while G.has_edge(u, v):
        u = random.randint(0, 1449)
        v = random.randint(0, 1449)

    # Ajouter l'arête entre les deux nœuds
    G.add_edge(u, v)

# Visualiser le graphe
nx.draw(G, with_labels=False)

import numpy as np
import pandas as pd

# Chargement des triplets du jeu de données FB15K
df = pd.read_csv('/content/sample_data/train.txt', sep='\t', header=None, names=['head', 'relation', 'tail'])

# Nombre de partitions souhaitées
k = 5

# Permutation aléatoire des indices des triplets
indices = np.random.permutation(df.index)

# Division des indices en k sous-ensembles
partitions = np.array_split(indices, k)

# Affichage des résultats
for i, part in enumerate(partitions):
    print(f"Partition {i}: {part}")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Chargement des triplets du jeu de données FB15K
df = pd.read_csv('/content/sample_data/train.txt', sep='\t', header=None, names=['head', 'relation', 'tail'])

# Nombre de partitions souhaitées
k = 5

# Permutation aléatoire des indices des triplets
indices = np.random.permutation(df.index)

# Division des indices en k sous-ensembles
partitions = np.array_split(indices, k)

# Affichage des résultats
fig, ax = plt.subplots(figsize=(8, 5))
ax.hist(partitions, bins=k, edgecolor='black')
ax.set_xlabel('Partition')
ax.set_ylabel('Nombre de triplets')
ax.set_title(f'{k} partitions de triplets')
plt.show()

import matplotlib.pyplot as plt

# Nombre total de triplets
total_triplets = 480000

# Nombre de triplets dans chaque partition
partition_sizes = [len(part) for part in partitions]

# Créer un histogramme des tailles de partition
partition_size = len(df) // k
partition_sizes = [partition_size] * k


# Ajouter des étiquettes d'axe et un titre
plt.xticks(range(4), ['Partition 0', 'Partition 1', 'Partition 2', 'Partition 3'])
plt.xlabel('Partition')
plt.ylabel('Nombre de triplets')
plt.title('Répartition des triplets dans les partitions')

# Ajouter une ligne pour la taille totale
plt.axhline(y=total_triplets/4, linestyle='--', color='r', label='Taille moyenne par partition')

# Afficher le graphique
plt.legend()
plt.show()

#Plot modularity
plt.plot(modularity, 'o')
plt.xlabel('# of clusters')
plt.ylabel('modularity')
plt.show()